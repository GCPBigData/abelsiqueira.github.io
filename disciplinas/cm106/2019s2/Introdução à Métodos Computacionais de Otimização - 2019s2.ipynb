{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "pkg\"activate .\"\n",
    "#=\n",
    "pkg\"add NLPModels\"\n",
    "pkg\"add SolverTools\"\n",
    "pkg\"add CUTEst\"\n",
    "pkg\"add SolverBenchmark\"\n",
    "pkg\"add Plots\"\n",
    "pkg\"add PyPlot\"\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg\"status\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução à Métodos Computacionais de Otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def. (Algoritmo):** Um algoritmo é uma coleção de instruções para realizar alguma tarefa específica. Segundo Donald Knuth (The Art of Computer Programming, v.1), ele deve satisfazer as seguintes condições, parafraseadas aqui:\n",
    "- **Finitude:** O algoritmo deve acabar em tempo finito;\n",
    "- **Bem definido:** As intruções devem ser claras e sem ambiguidade;\n",
    "- **Entrada:** O algoritmo tem zero ou mais entradas, que são valores determinados antes do algoritmo começar. Essas entradas são especificados a partir de conjuntos de objetos;\n",
    "- **Saída:** O algoritmo tem uma ou mais saídas, que são quantidades relacionadas com as entradas;\n",
    "- **Eficácia/Computabilidade:** As operações feitas no algoritmo devem ser suficientemente básicas para que a princípio possam ser executadas por uma pessoa num espaço finito e tempo finito com papel e caneta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um **método** é menos restrito, e uma **implementação** é a realização computacional de um algoritmo.\n",
    "\n",
    "Pode-se dizer que, dado um problema, existem vários métodos para resolvê-lo, para cada método, existem vários algoritmos que o usam, para cada algoritmo, várias implementações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condições de paradas\n",
    "\n",
    "Um algoritmo de otimização será um algoritmo com o objetivo de encontrar pontos críticos de um problema informado pelo usuário. Alguns algoritmos podem verificar condições de segunda ordem, mas em geral estaremos olhando apenas para as condições de primeira ordem. No caso irrestrito continuamente diferenciável, isso quer dizer $\\nabla f(\\overline{x}) = 0$. Essa condição precisa ser relaxada para obtermos uma condição razoável de parada de sucesso, isto é, uma condição que diz que o ponto encontrado está suficientemente próximo de ser um ponto crítico do problema. Para tanto, utilizaremos uma *tolerância* para a condição de primeira ordem: Buscaremos um ponto onde o gradiente está suficientemente próximo de zero, por exemplo verificando\n",
    "$\\Vert \\nabla f(x_k)\\Vert < \\varepsilon$ ou\n",
    "$\\Vert \\nabla f(x_k)\\Vert < \\varepsilon \\Vert \\nabla f(x_0)\\Vert$, ou ainda, uma combinação das duas:\n",
    "$$\\Vert \\nabla f(x_k)\\Vert < \\varepsilon_a + \\varepsilon_r \\Vert \\nabla f(x_0)\\Vert.$$\n",
    "\n",
    "Os métodos de otimização costumam ter alguma garantia de convergência perto da solução, ou em alguns casos, de gerar pontos de acumulação críticos. No entanto, pode acontecer de o método demorar demais na busca de uma solução, ou de encontrar um obstáculo que não pode ultrapassar. Para evitar que o seu programa tenha um *bug*, é preciso uma combinação de entendimento computacional e teórico do algoritmo para criar condições de parada adicionais. As mais comuns estão a seguir:\n",
    "\n",
    "- Máximo de iterações, de avaliação de funções, de tempo decorrido, etc.\n",
    "- Insatisfação de alguma condição teórica para o funcionamento do método, e.g., positividade da Hessiana em Newton, geração de uma direção que não seja de descida;\n",
    "- Acontecimento computacional que não deveria acontecer na matemática exata, mas que ocorre devido ao uso de ponto flutuante, .e.g, má condicionamento da Hessiana, o passo de Armijo vira zero, divisão por algum número muito próximo de zero, a direção fica muito perto de ser ortogonal;\n",
    "- Acontecimentos puramente computacionais, e.g., falta de memória;\n",
    "\n",
    "Muitos problemas só são percebidos após a implementação é iniciada, por isso é preciso ficar atento e implementar testes para o seu código.\n",
    "\n",
    "## Indicador de Saída - Status\n",
    "\n",
    "O comum nessas situações de parada é indicar com alguma variável o que aconteceu. No passado, o costume era retornar $0$ se tudo correu bem, $>0$ para paradas previstas, e $<0$ para exceções.\n",
    "Hoje em dia podemos retornar texto ou símobolos descrevendo a saída, por exemplo, `\"sucesso\"`, `\"máximo de iterações\"`, `\"máximo de tempo\"`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JuliaSmoothOptimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JuliaSmoothOptimizers](jso.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O [JuliaSmoothOptimizers](https://github.com/JuliaSmoothOptimizers) é um framework de desenvolvimento de solvers em Julia. Ele ajuda a definir um modelo, criar métodos de otimização, compará-los, e gerar gráficos e tabelas. Vamos apresentar as partes mais básicas do framework para não sobrecarregar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPModels.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLPModels\n",
    "\n",
    "f(x) = (x[1] - 1)^2 + 4 * (x[2] - x[1]^2)^2\n",
    "x₀ = [-1.2; 1.0]\n",
    "nlp = ADNLPModel(f, x₀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj(nlp, x₀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj(nlp, [0.0; 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(nlp, x₀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(nlp, [1.0; 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hess(nlp, x₀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hess(nlp, [1.0; 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "Symmetric(hess(nlp, x₀), :L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hx = Symmetric(hess(nlp, [1.0; 1.0]), :L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen(Hx).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de Newton Puro\n",
    "\n",
    "1. Dados $x_0$, $\\varepsilon > 0$, $k = 0$, $k_\\max$.\n",
    "2. Enquanto $\\Vert\\nabla f(x_k)\\Vert > \\varepsilon$\n",
    "    1. Calcule $d_k$ resolvendo o sistema $\\nabla^2 f(x_k) d = -\\nabla f(x_k)$\n",
    "    2. Calcule $x_{k+1} = x_k + d_k$\n",
    "    3. $k = k + 1$\n",
    "    4. Teste outras condições de parada e vá à 4 se alguma for satisfeita\n",
    "3. Fim do Enquanto\n",
    "4. Saída: $x_k$, $f(x_k)$, $\\Vert\\nabla f(x_k)\\Vert$, Tempo, Iterações, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "pyplot(size=(400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = range(-2.0, 2.0, length=100)\n",
    "yg = range(-1.0, 3.0, length=100)\n",
    "contour(xg, yg, (x,y) -> obj(nlp, [x;y]), levels=100, leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x) = grad(nlp, x)\n",
    "H(x) = Symmetric(hess(nlp, x), :L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(ones(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nlp.meta.x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = -H(x) \\ g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(xg, yg, (x,y) -> obj(nlp, [x;y]), levels=50, leg=false)\n",
    "scatter!([x[1]], [x[2]], c=:red, ms=3, leg=false)\n",
    "plot!([x[1], x[1] + d[1]], [x[2], x[2] + d[2]], c=:blue, l=:arrow)\n",
    "scatter!([x[1] + d[1]], [x[2] + d[2]], c=:blue, ms=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = contour(xg, yg, (x,y) -> obj(nlp, [x;y]), levels=50, leg=false)\n",
    "x = copy(nlp.meta.x0)\n",
    "scatter!([x[1]], [x[2]], c=:red, ms=3, leg=false)\n",
    "d = -H(x) \\ g(x)\n",
    "plot!([x[1], x[1] + d[1]], [x[2], x[2] + d[2]], c=:blue, l=:arrow)\n",
    "scatter!([x[1] + d[1]], [x[2] + d[2]], c=:blue, ms=3)\n",
    "while norm(d) > 1e-4\n",
    "    x += d\n",
    "    d = -H(x) \\ g(x)\n",
    "    plot!([x[1], x[1] + d[1]], [x[2], x[2] + d[2]], c=:blue, l=:arrow)\n",
    "    scatter!([x[1] + d[1]], [x[2] + d[2]], c=:blue, ms=3)\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SolverTools\n",
    "\n",
    "function newton_puro(nlp :: AbstractNLPModel;\n",
    "                     tol = 1e-6, kmax = 10000, max_time = 30.0\n",
    "                    )\n",
    "    x = copy(nlp.meta.x0)\n",
    "    \n",
    "    g(x) = grad(nlp, x)\n",
    "    H(x) = Symmetric(hess(nlp, x), :L)\n",
    "    t0 = time()\n",
    "    Δt = 0.0\n",
    "    \n",
    "    k = 0\n",
    "    gx = g(x)\n",
    "    while !(norm(gx) < tol || k > kmax || Δt > max_time)\n",
    "        d = -H(x) \\ gx\n",
    "        x = x + d\n",
    "        gx = g(x)\n",
    "        k = k + 1\n",
    "        Δt = time() - t0 # Tempo decorrido\n",
    "    end\n",
    "    \n",
    "    status = :unknown\n",
    "    if norm(gx) < tol\n",
    "        status = :first_order\n",
    "    elseif k > kmax\n",
    "        status = :max_iter\n",
    "    elseif Δt > max_time\n",
    "        status = :max_time\n",
    "    end\n",
    "    \n",
    "    # Parte do SolverTools.jl\n",
    "    return GenericExecutionStats(status, nlp, solution=x, objective=obj(nlp, x),\n",
    "                                 dual_feas=norm(gx), iter=k, elapsed_time=Δt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = newton_puro(nlp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletâneas de problemas\n",
    "\n",
    "Quando algoritmos eram criados há algumas décadas, era necessário criar também algumas funções para comparar esses algoritmos. Essas funções costumavam ser comparilhadas entre autores, para que todos pudessem fazer testes computacionais. Alguns artigos foram publicados descrevendo conjuntos de problemas que poderiam ser úteis em contextos específicos. Eis alguns:\n",
    "- W. Hock, K. Schittkowski. **Test Examples for Nonlinear Programming Codes**, Springer, 1981\n",
    "- Jorge J. Moré, Burton S. Garbow, and Kenneth E. Hillstrom. 1981. **Testing Unconstrained Optimization Software.** ACM Trans. Math. Softw. 7, 1 (1981), 17-41. DOI: https://doi.org/10.1145/355934.355936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em 1995, com a publicação\n",
    "\n",
    "- I. Bongartz, A. R. Conn, N. I. M. Gould, and Ph. L. Toint. CUTE: Constrained\n",
    "and Unconstrained Testing Environment. ACM Transactions on Mathematical\n",
    "Software, 21(1):123–160, 1995.\n",
    "\n",
    "vários problemas foram colecionados num software que dava acesso à todas as qualidades de um problema de programação não-linear: função objetivo, restrições, gradientes, Hessianas, ponto inicial, etc. Essa biblioteca de testes, **CUTE** teve duas versões seguintes: **CUTEr** e **CUTEst**. Esta última, a mais atual, têm uma interface em Julia, que podemos acessar sem muita dificuldade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUTEst.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar o `ADNLPModel` nos permite criar exemplos rápido, mas para testes computacionais de verdade, precisamos de uma biblioteca de testes. A mais usada está disponível no `CUTEst.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUTEst\n",
    "\n",
    "nlp = CUTEstModel(\"ROSENBR\")\n",
    "out = newton_puro(nlp)\n",
    "finalize(nlp) # Necessário para fechar o problema\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `CUTEst` tem todas as informações de problema armazenadas internamente, incluindo o ponto inicial, as funções e como calcular suas derivadas. Para acessá-las, é necessário saber o nome do problema. Como não sabemos isso, escolhemos problemas pelas suas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemas = CUTEst.select(max_var=2, max_con=0, only_free_var=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"HILBERTA\")\n",
    "output = newton_puro(nlp)\n",
    "println(output)\n",
    "finalize(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"HIMMELBB\")\n",
    "local output\n",
    "try\n",
    "    output = newton_puro(nlp)\n",
    "catch ex\n",
    "    println(\"EXCECAO\")\n",
    "finally\n",
    "    finalize(nlp)\n",
    "end\n",
    "println(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gerador = (CUTEstModel(p) for p in problemas)\n",
    "df = solve_problems(newton_puro, gerador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar uma implementação do método do Gradiente com busca de Armijo\n",
    "\n",
    "## Método do Gradiente com Busca Inexata\n",
    "\n",
    "1. Dados $x_0$, $\\varepsilon > 0$, $k = 0$, $k_\\max \\in \\mathbb{N}$, $\\alpha, \\sigma \\in (0,1)$.\n",
    "2. Enquanto $\\Vert\\nabla f(x_k)\\Vert > \\varepsilon$\n",
    "    1. $d_k = -\\nabla f(x_k)$\n",
    "    2. Defina $t_k$ como o primeiro valor da sequência\n",
    "        $\\{1,\\sigma,\\sigma^2,\\sigma^3,\\dots\\}$ tal que\n",
    "$$f(x_k + t_k d_k) < f(x_k) + \\alpha t_k \\nabla f(x_k)^Td_k. $$\n",
    "    3. Calcule $x_{k+1} = x_k + t_k d_k$\n",
    "    4. $k = k + 1$\n",
    "    5. Teste outras condições de parada e vá à 4 se alguma for satisfeita\n",
    "3. Fim do Enquanto\n",
    "4. Saída: $x_k$, $f(x_k)$, $\\Vert\\nabla f(x_k)\\Vert$, Tempo, Iterações, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradiente_armijo(nlp;\n",
    "                          tol = 1e-6, kmax = 10000, max_time = 30.0,\n",
    "                          α = 1e-4\n",
    "                          )\n",
    "    x = copy(nlp.meta.x0)\n",
    "    \n",
    "    f(x) = obj(nlp, x)\n",
    "    g(x) = grad(nlp, x)\n",
    "    \n",
    "    t0 = time()\n",
    "    Δt = 0.0\n",
    "    \n",
    "    status = :unknown\n",
    "    k = 0\n",
    "    fx = f(x)\n",
    "    gx = g(x)\n",
    "    while !(norm(gx) < tol || k > kmax || Δt > max_time)\n",
    "        d = -gx\n",
    "        t = 1.0\n",
    "        xt = x + d\n",
    "        ft = f(xt)\n",
    "        while ft ≥ fx + α * t * dot(d, gx)\n",
    "            t = 0.9t\n",
    "            xt = x + t * d\n",
    "            ft = f(xt)\n",
    "            if t < 1e-16\n",
    "                status = :stalled\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        if status == :stalled\n",
    "            break\n",
    "        end\n",
    "        x = xt \n",
    "        fx = ft\n",
    "        gx = g(x)\n",
    "        k = k + 1\n",
    "        Δt = time() - t0 # Tempo decorrido\n",
    "    end\n",
    "    \n",
    "    if norm(gx) < tol\n",
    "        status = :first_order\n",
    "    elseif k > kmax\n",
    "        status = :max_iter\n",
    "    elseif Δt > max_time\n",
    "        status = :max_time\n",
    "    end\n",
    "    \n",
    "    # Parte do SolverTools.jl\n",
    "    return GenericExecutionStats(status, nlp, solution=x, objective=obj(nlp, x),\n",
    "                                 dual_feas=norm(gx), iter=k, elapsed_time=Δt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"ROSENBR\")\n",
    "output = gradiente_armijo(nlp, α = 1e-2, kmax=100_000)\n",
    "finalize(nlp)\n",
    "println(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"HIMMELBB\")\n",
    "output = gradiente_armijo(nlp)\n",
    "finalize(nlp)\n",
    "println(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"HIMMELBB\")\n",
    "output = newton_puro(nlp)\n",
    "finalize(nlp)\n",
    "println(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparações entre algoritmos de otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfil de Desempenho\n",
    "\n",
    "O perfil de desempenho é um gráfico de comparação de algoritmos útil quando existe uma troca de eficiência e robustez. Em geral ele não é muito útil para algoritmos que sempre convergem.\n",
    "\n",
    "A ideia do perfil de desempenho é \"normalizar\" a comparação. Problemas menores tendem a ser resolvidos mais rápido, enquanto problemas maiores podem demorar vários minutos. Nessa situação, 1 minuto de diferença pode ser muito ou pouco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "problemas = CUTEst.select(max_var=2, max_con=0, only_free_var=true)\n",
    "gerador = (CUTEstModel(p) for p in problemas)\n",
    "solvers = Dict(:newton_puro => newton_puro, :gradiente_armijo => gradiente_armijo)\n",
    "stats = bmark_solvers(solvers, gerador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SolverBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function custo(df)\n",
    "    falha = (df.status .!= :first_order)\n",
    "    return falha * Inf + df.elapsed_time\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_profile(stats, custo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_join = join(stats, [:status, :objective], invariant_cols=[:name])\n",
    "print(df_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f \\le f_{\\min} + 10^{-1}|f_{\\min}| + 10^{-6} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmin = min.(stats[:newton_puro].objective, stats[:gradiente_armijo].objective)\n",
    "function custo(df)\n",
    "    falha = df.objective .≤ fmin + 1e-1 * abs.(fmin) .+ 1e-6\n",
    "    return falha * Inf + df.elapsed_time\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_profile(stats, custo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton com Armijo e salvaguarda\n",
    "\n",
    "- Gosto: $ \\nabla^2 f(x^k) d = -\\nabla f(x^k), $\n",
    "- Preciso: $ \\nabla^2 f(x^k)$ definida positiva\n",
    "- Se não for, uso $d = -\\nabla f(x^k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_armijo(nlp;\n",
    "                       tol = 1e-6, kmax = 10000, max_time = 30.0\n",
    "                       )\n",
    "    x = copy(nlp.meta.x0)\n",
    "    \n",
    "    f(x) = obj(nlp, x)\n",
    "    g(x) = grad(nlp, x)\n",
    "    H(x) = Symmetric(hess(nlp, x), :L)\n",
    "    \n",
    "    t0 = time()\n",
    "    Δt = 0.0\n",
    "    \n",
    "    status = :unknown\n",
    "    k = 0\n",
    "    fx = f(x)\n",
    "    gx = g(x)\n",
    "    Hx = H(x)\n",
    "    while !(norm(gx) < tol || k > kmax || Δt > max_time)\n",
    "        d = -Hx \\ gx\n",
    "        if dot(d, gx) > 0\n",
    "            d = -gx\n",
    "        end\n",
    "        t = 1.0\n",
    "        xt = x + d\n",
    "        ft = f(xt)\n",
    "        while ft ≥ fx + 1e-4 * t * dot(d, gx)\n",
    "            t = 0.9t\n",
    "            xt = x + t * d\n",
    "            ft = f(xt)\n",
    "            if t < 1e-16\n",
    "                status = :stalled\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        if status == :stalled\n",
    "            break\n",
    "        end\n",
    "        x = xt \n",
    "        fx = ft\n",
    "        gx = g(x)\n",
    "        Hx = H(x)\n",
    "        k = k + 1\n",
    "        Δt = time() - t0 # Tempo decorrido\n",
    "    end\n",
    "    \n",
    "    if norm(gx) < tol\n",
    "        status = :first_order\n",
    "    elseif k > kmax\n",
    "        status = :max_iter\n",
    "    elseif Δt > max_time\n",
    "        status = :max_time\n",
    "    end\n",
    "    \n",
    "    return GenericExecutionStats(status, nlp, solution=x, objective=obj(nlp, x),\n",
    "                                 dual_feas=norm(gx), iter=k, elapsed_time=Δt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"ROSENBR\")\n",
    "try\n",
    "    output = newton_armijo(nlp)\n",
    "    print(output)\n",
    "catch ex\n",
    "    println(ex)\n",
    "finally\n",
    "    finalize(nlp)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solvers = Dict(:newton_puro => newton_puro,\n",
    "               :newton_armijo => newton_armijo,\n",
    "               :gradiente_armijo => gradiente_armijo)\n",
    "stats = bmark_solvers(solvers, gerador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats[:newton_armijo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_profile(stats, custo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sort(rand(100))\n",
    "y = [xi + randn() * 0.3 < 0.7 ? 0.0 : 1.0 for xi in x]\n",
    "scatter(x, y, leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ(t) = 1 / (1 + exp(-t))\n",
    "h(β, x) = σ(β[1] + β[2] * x)\n",
    "scatter(x, y, leg=false)\n",
    "plot!(x -> h([-5.0; 10.0], x), c=:red, lw=2)\n",
    "plot!(x -> h([-3.0; 12.0], x), c=:green, lw=2)\n",
    "plot!(x -> h([-3.0; 4.0], x), c=:yellow, lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ -y_i \\ln h(\\beta, x_i) - (1 - y_i) \\ln \\Big(1 - h(\\beta, x_i)\\Big) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J(β) = sum(-y[i] * log(h(β, x[i])) - (1 - y[i]) * log(1 - h(β, x[i])) for i = 1:100)\n",
    "nlp = ADNLPModel(J, ones(2))\n",
    "output = newton_puro(nlp)\n",
    "println(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "β = output.solution\n",
    "scatter(x, y, leg=false)\n",
    "plot!(x -> h(β, x), c=:red, lw=2)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
