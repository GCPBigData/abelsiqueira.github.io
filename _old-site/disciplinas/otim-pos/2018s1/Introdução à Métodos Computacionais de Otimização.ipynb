{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução à Métodos Computacionais de Otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def. (Algoritmo):** Um algoritmo é uma coleção de instruções para realizar alguma tarefa específica. Segundo Donald Knuth (The Art of Computer Programming, v.1), ele deve satisfazer as seguintes condições, parafraseadas aqui:\n",
    "- **Finitude:** O algoritmo deve acabar em tempo finito;\n",
    "- **Bem definido:** As intruções devem ser claras e sem ambiguidade;\n",
    "- **Entrada:** O algoritmo tem zero ou mais entradas, que são valores determinados antes do algoritmo começar. Essas entradas são especificados a partir de conjuntos de objetos;\n",
    "- **Saída:** O algoritmo tem uma ou mais saídas, que são quantidades relacionadas com as entradas;\n",
    "- **Eficácia/Computabilidade:** As operações feitas no algoritmo devem ser suficientemente básicas para que a princípio possam ser executadas por uma pessoa num espaço finito e tempo finito com papel e caneta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um **método** é menos restrito, e uma **implementação** é a realização computacional de um algoritmo.\n",
    "\n",
    "Pode-se dizer que, dado um problema, existem vários métodos para resolvê-lo, para cada método, existem vários algoritmos que o usam, para cada algoritmo, várias implementações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condições de paradas\n",
    "\n",
    "Um algoritmo de otimização será um algoritmo com o objetivo de encontrar pontos críticos de um problema informado pelo usuário. Alguns algoritmos podem verificar condições de segunda ordem, mas em geral estaremos olhando apenas para as condições de primeira ordem. No caso irrestrito continuamente diferenciável, isso quer dizer $\\nabla f(\\overline{x}) = 0$. Essa condição precisa ser relaxada para obtermos uma condição razoável de parada de sucesso, isto é, uma condição que diz que o ponto encontrado está suficientemente próximo de ser um ponto crítico do problema. Para tanto, utilizaremos uma *tolerância* para a condição de primeira ordem: Buscaremos um ponto onde o gradiente está suficientemente próximo de zero, por exemplo verificando\n",
    "$\\Vert \\nabla f(x_k)\\Vert < \\varepsilon$ ou\n",
    "$\\Vert \\nabla f(x_k)\\Vert < \\varepsilon \\Vert \\nabla f(x_0)\\Vert$, ou ainda, uma combinação das duas:\n",
    "$$\\Vert \\nabla f(x_k)\\Vert < \\varepsilon_a + \\varepsilon_r \\Vert \\nabla f(x_0)\\Vert.$$\n",
    "\n",
    "Os métodos de otimização costumam ter alguma garantia de convergência perto da solução, ou em alguns casos, de gerar pontos de acumulação críticos. No entanto, pode acontecer de o método demorar demais na busca de uma solução, ou de encontrar um obstáculo que não pode ultrapassar. Para evitar que o seu programa tenha um *bug*, é preciso uma combinação de entendimento computacional e teórico do algoritmo para criar condições de parada adicionais. As mais comuns estão a seguir:\n",
    "\n",
    "- Máximo de iterações, de avaliação de funções, de tempo decorrido, etc.\n",
    "- Insatisfação de alguma condição teórica para o funcionamento do método, e.g., positividade da Hessiana em Newton, geração de uma direção que não seja de descida;\n",
    "- Acontecimento computacional que não deveria acontecer na matemática exata, mas que ocorre devido ao uso de ponto flutuante, .e.g, má condicionamento da Hessiana, o passo de Armijo vira zero, divisão por algum número muito próximo de zero, a direção fica muito perto de ser ortogonal;\n",
    "- Acontecimentos puramente computacionais, e.g., falta de memória;\n",
    "\n",
    "Muitos problemas só são percebidos após a implementação é iniciada, por isso é preciso ficar atento e implementar testes para o seu código.\n",
    "\n",
    "## Indicador de Saída - Exit Flag\n",
    "\n",
    "O comum nessas situações de parada é indicar com alguma variável o que aconteceu. No passado, o costume era retornar $0$ se tudo correu bem, $>0$ para paradas previstas, e $<0$ para exceções.\n",
    "Hoje em dia podemos retornar texto descrevendo a saída, por exemplo, `\"sucesso\"`, `\"máximo de iterações\"`, `\"máximo de tempo\"`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de Newton Puro\n",
    "\n",
    "1. Dados $x_0$, $\\varepsilon > 0$, $k = 0$, $k_\\max$.\n",
    "2. Enquanto $\\Vert\\nabla f(x_k)\\Vert > \\varepsilon$\n",
    "    1. Calcule $d_k$ resolvendo o sistema $\\nabla^2 f(x_k) d = -\\nabla f(x_k)$\n",
    "    2. Calcule $x_{k+1} = x_k + d_k$\n",
    "    3. $k = k + 1$\n",
    "    4. Teste outras condições de parada e vá à 4 se alguma for satisfeita\n",
    "3. Fim do Enquanto\n",
    "4. Saída: $x_k$, $f(x_k)$, $\\Vert\\nabla f(x_k)\\Vert$, Tempo, Iterações, exitflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2\n",
    "g(x) = [2 * (x[1] - 1) - 400 * x[1] * (x[2] - x[1]^2);\n",
    "         200 * (x[2] - x[1]^2)]\n",
    "H(x) = [2 - 400 * (x[2] - x[1]^2) + 800 * x[1]^2   -400 * x[1];\n",
    "        -400 * x[1]  200]\n",
    "\n",
    "# Ponto inicial\n",
    "x = [-1.2; 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(linspace(-2, 2, 100), linspace(-1, 2, 100), (x,y) -> f([x;y]), levels=50)\n",
    "scatter!([x[1]], [x[2]], c=:red, ms=3, leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = -H(x) \\ g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(linspace(-2, 2, 100), linspace(-1, 2, 100), (x,y) -> f([x;y]), levels=50)\n",
    "scatter!([x[1]], [x[2]], c=:red, ms=3, leg=false)\n",
    "dn = 0.25 * d / norm(d)\n",
    "plot!([x[1], x[1] + dn[1]], [x[2], x[2] + dn[2]], c=:blue, l=:arrow)\n",
    "scatter!([x[1] + d[1]], [x[2] + d[2]], c=:blue, ms=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-1.2; 1.0]\n",
    "contour(linspace(-2, 2, 100), linspace(-3.5, 2, 100), (x,y) -> f([x;y]), levels=50)\n",
    "scatter!([x[1]], [x[2]], c=:red, ms=3, leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter!([x[1]], [x[2]], c=:red, ms=3)\n",
    "d = -H(x) \\ g(x)\n",
    "dn = 0.25 * d / norm(d)\n",
    "plot!([x[1], x[1] + dn[1]], [x[2], x[2] + dn[2]], c=:blue, l=:arrow)\n",
    "x = x + d\n",
    "scatter!([x[1]], [x[2]], c=:blue, ms=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_puro(f, g, H, x;\n",
    "                     tol = 1e-6, kmax = 1000, max_evals = 10_000, max_time = 30.0\n",
    "                    )\n",
    "    # Implementar em sala\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_puro(f, g, H, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = x[1]^4 + x[2]^4\n",
    "g(x) = 4 * [x[1]^3; x[2]^3]\n",
    "H(x) = 12 * [x[1]  0.0; 0.0  x[2]]\n",
    "x = ones(2)\n",
    "newton_puro(f, g, H, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_puro(f, g, H, x, tol=1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletâneas de problemas\n",
    "\n",
    "Quando algoritmos eram criados há algumas décadas, era necessário criar também algumas funções para comparar esses algoritmos. Essas funções costumavam ser comparilhadas entre autores, para que todos pudessem fazer testes computacionais. Alguns artigos foram publicados descrevendo conjuntos de problemas que poderiam ser úteis em contextos específicos. Eis alguns:\n",
    "- W. Hock, K. Schittkowski. **Test Examples for Nonlinear Programming Codes**, Springer, 1981\n",
    "- Jorge J. Moré, Burton S. Garbow, and Kenneth E. Hillstrom. 1981. **Testing Unconstrained Optimization Software.** ACM Trans. Math. Softw. 7, 1 (1981), 17-41. DOI: https://doi.org/10.1145/355934.355936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em 1995, com a publicação\n",
    "\n",
    "- I. Bongartz, A. R. Conn, N. I. M. Gould, and Ph. L. Toint. CUTE: Constrained\n",
    "and Unconstrained Testing Environment. ACM Transactions on Mathematical\n",
    "Software, 21(1):123–160, 1995.\n",
    "\n",
    "vários problemas foram colecionados num software que dava acesso à todas as qualidades de um problema de programação não-linear: função objetivo, restrições, gradientes, Hessianas, ponto inicial, etc. Essa biblioteca de testes, **CUTE** teve duas versões seguintes: **CUTEr** e **CUTEst**. Esta última, a mais atual, têm uma interface em Julia, que podemos acessar sem muita dificuldade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLPModels.jl e CUTEst.jl\n",
    "\n",
    "Vamos utilizar dois pacotes, parte do [JuliaSmoothOptimizers](https://github.com/JuliaSmoothOptimizers/), para criar/obter nossas funções $f$, $\\nabla f$ e $\\nabla^2 f$.\n",
    "\n",
    "**NLPModels.jl** é um pacote para abstração de modelos de PNL. Em particular, um modelo que nos interessa é o `ADNLPModel`, que vai receber $f$ e calcular $\\nabla f$ e $\\nabla^2 f$ para nós."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLPModels\n",
    "\n",
    "f(x) = (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2\n",
    "\n",
    "nlp = ADNLPModel(f, [-1.2; 1.0]) # Cria o modelo e guarda tudo na variável nlp\n",
    "\n",
    "x = nlp.meta.x0 # Obtém o x0 a partir da variável nlp\n",
    "obj(nlp, x) # Calcula o valor da função objetivo a partir do nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(2)\n",
    "obj(nlp, x) - f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(nlp, x) # Calcula o gradiente usando diferenciação automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hess(nlp, x) # Calcula a Hessiana usando diferenciação automática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por design, a Hessiana retornada é só a metade inferior. Num software mais avançado, utilizaríamos só a metade inferior para, por exemplo, calcular Cholesky, mas aqui vamos usar a Hessiana inteira para resolver sistemas sem preocupações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Symmetric(hess(nlp, x), :L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No lugar de fazer nossa função depender de `f, g, H, x`, podemos colocar como entrada apenas `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_puro(nlp; kwargs...)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = ADNLPModel(x->x[1]^4 + x[2]^4, ones(2))\n",
    "newton_puro(nlp, tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = ADNLPModel(x->sum(x.^4), ones(10))\n",
    "newton_puro(nlp, tol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar o `ADNLPModel` nos permite criar exemplos rápido, mas para testes computacionais de verdade, precisamos de uma biblioteca de testes. A mais usada está disponível no `CUTEst.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUTEst\n",
    "\n",
    "nlp = CUTEstModel(\"ROSENBR\")\n",
    "out = newton_puro(nlp)\n",
    "finalize(nlp) # Necessário para fechar o problema\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `CUTEst` tem todas as informações de problema armazenadas internamente, incluindo o ponto inicial, as funções e como calcular suas derivadas. Para acessá-las, é necessário saber o nome do problema. Como não sabemos isso, escolhemos problemas pelas suas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemas = CUTEst.select(max_var=2, max_con=0, only_free_var=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"HILBERTA\")\n",
    "out = newton_puro(nlp)\n",
    "finalize(nlp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"HIMMELBB\")\n",
    "out = newton_puro(nlp)\n",
    "finalize(nlp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemas = CUTEst.select(max_var = 2, max_con = 0, only_free_var=true)\n",
    "sort!(problemas)\n",
    "mtds = [newton_puro]\n",
    "\n",
    "@printf(\"%-10s | \", \"Problema\")\n",
    "for mtd in mtds\n",
    "    @printf(\"%27s | \", mtd)\n",
    "end\n",
    "println(\"\")\n",
    "@printf(\"%10s | \", \"\")\n",
    "for mtd in mtds\n",
    "    @printf(\"%9s  %8s  %6s | \",\n",
    "            \"f(x)\", \"Δt\", \"Σ#f\")\n",
    "end\n",
    "println(\"\")\n",
    "\n",
    "np, nmtds = length(problemas), length(mtds)\n",
    "fx_matriz = zeros(np, nmtds)\n",
    "Δt_matriz = zeros(np, nmtds)\n",
    "Σf_matriz = zeros(Int, np, nmtds)\n",
    "for (i,p) in enumerate(problemas)\n",
    "    @printf(\"%-10s | \", p)\n",
    "    nlp = CUTEstModel(p)\n",
    "    try\n",
    "        for (j,mtd) in enumerate(mtds)\n",
    "            try\n",
    "                reset!(nlp)\n",
    "                x, fx, normgx, Δt, iters, exitflag = mtd(nlp, max_evals = 10_000, max_time = 3.0)\n",
    "                Σf = sum_counters(nlp)\n",
    "                @printf(\"%+8.2e  %8.2e  %6d | \", fx, Δt, Σf)\n",
    "                fx_matriz[i,j] = fx\n",
    "                Δt_matriz[i,j] = Δt\n",
    "                Σf_matriz[i,j] = Σf\n",
    "            catch ex\n",
    "                @printf(\"%27s | \", \"ERROR\")\n",
    "                fx_matriz[i,j] = Inf\n",
    "                Δt_matriz[i,j] = Inf\n",
    "                Σf_matriz[i,j] = 2^16\n",
    "            end\n",
    "        end\n",
    "        println(\"\")\n",
    "    catch ex\n",
    "        println(ex)\n",
    "    finally\n",
    "        finalize(nlp)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar uma implementação do método do Gradiente com busca de Armijo\n",
    "\n",
    "## Método do Gradiente com Busca Inexata\n",
    "\n",
    "1. Dados $x_0$, $\\varepsilon > 0$, $k = 0$, $k_\\max \\in \\mathbb{N}$, $\\alpha, \\sigma \\in (0,1)$.\n",
    "2. Enquanto $\\Vert\\nabla f(x_k)\\Vert > \\varepsilon$\n",
    "    1. $d_k = -\\nabla f(x_k)$\n",
    "    2. Defina $t_k$ como o primeiro valor da sequência\n",
    "        $\\{1,\\sigma,\\sigma^2,\\sigma^3,\\dots\\}$ tal que\n",
    "$$f(x_k + t_k d_k) < f(x_k) + \\alpha t_k \\nabla f(x_k)^Td_k. $$\n",
    "    3. Calcule $x_{k+1} = x_k + t_k d_k$\n",
    "    4. $k = k + 1$\n",
    "    5. Teste outras condições de parada e vá à 4 se alguma for satisfeita\n",
    "3. Fim do Enquanto\n",
    "4. Saída: $x_k$, $f(x_k)$, $\\Vert\\nabla f(x_k)\\Vert$, Tempo, Iterações, exitflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradiente_inexato(nlp;\n",
    "                           tol = 1e-6, kmax = 1000, max_evals = 10_000, max_time = 30.0\n",
    "                           )\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"HILBERTA\")\n",
    "out = gradiente_inexato(nlp)\n",
    "finalize(nlp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"HIMMELBB\")\n",
    "out = gradiente_inexato(nlp)\n",
    "finalize(nlp)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparações entre algoritmos de otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemas = CUTEst.select(max_var = 2, max_con = 0, only_free_var=true)\n",
    "sort!(problemas)\n",
    "mtds = [gradiente_inexato, newton_puro]\n",
    "\n",
    "@printf(\"%-10s | \", \"Problema\")\n",
    "for mtd in mtds\n",
    "    @printf(\"%27s | \", mtd)\n",
    "end\n",
    "println(\"\")\n",
    "@printf(\"%10s | \", \"\")\n",
    "for mtd in mtds\n",
    "    @printf(\"%9s  %8s  %6s | \",\n",
    "            \"f(x)\", \"Δt\", \"Σ#f\")\n",
    "end\n",
    "println(\"\")\n",
    "\n",
    "np, nmtds = length(problemas), length(mtds)\n",
    "fx_matriz = zeros(np, nmtds)\n",
    "Δt_matriz = zeros(np, nmtds)\n",
    "Σf_matriz = zeros(Int, np, nmtds)\n",
    "for (i,p) in enumerate(problemas)\n",
    "    @printf(\"%-10s | \", p)\n",
    "    nlp = CUTEstModel(p)\n",
    "    try\n",
    "        for (j,mtd) in enumerate(mtds)\n",
    "            try\n",
    "                reset!(nlp)\n",
    "                x, fx, normgx, Δt, iters, exitflag = mtd(nlp, max_evals = 10_000, max_time = 3.0)\n",
    "                Σf = sum_counters(nlp)\n",
    "                @printf(\"%+8.2e  %8.2e  %6d | \", fx, Δt, Σf)\n",
    "                fx_matriz[i,j] = fx\n",
    "                Δt_matriz[i,j] = Δt\n",
    "                Σf_matriz[i,j] = Σf\n",
    "            catch ex\n",
    "                @printf(\"%27s | \", \"ERROR\")\n",
    "                fx_matriz[i,j] = Inf\n",
    "                Δt_matriz[i,j] = Inf\n",
    "                Σf_matriz[i,j] = 2^16\n",
    "                throw(ex)\n",
    "            end\n",
    "        end\n",
    "        println(\"\")\n",
    "    catch ex\n",
    "        println(ex)\n",
    "    finally\n",
    "        finalize(nlp)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além de verificar o tempo ou avaliações de função, devemos garantir que os métodos funcionaram. Uma maneira de verificar isso, neste caso, é decidir que um algoritmo convergiu para a solução ótima se\n",
    "$$ f \\leq f_{\\min} + \\left\\vert f_{\\min}\\right\\vert \\varepsilon_{R} + \\varepsilon_{A},$$\n",
    "onde $f$ é o valor de função de um algoritmo, $f_{\\min}$ é o menor valor de função encontrado por qualquer algoritmo, e $\\varepsilon_R$ e $\\varepsilon_A$ são tolerâncias. Por exemplo, $\\varepsilon_{R} = 10^{-3}$\n",
    "e $\\varepsilon_{A} = 10^{-6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@printf(\"%-10s | \", \"Problema\")\n",
    "for mtd in mtds\n",
    "    @printf(\"%27s | \", mtd)\n",
    "end\n",
    "println(\"\")\n",
    "@printf(\"%10s | \", \"\")\n",
    "for mtd in mtds\n",
    "    @printf(\"%9s  %8s  %6s | \",\n",
    "            \"f(x)\", \"Δt\", \"Σ#f\")\n",
    "end\n",
    "println(\"\")\n",
    "\n",
    "epsR, epsA = 1e-3, 1e-6\n",
    "for i = 1:np\n",
    "    @printf(\"%-10s | \", problemas[i])\n",
    "    fmin = minimum(fx_matriz[i,:])\n",
    "    for j = 1:nmtds\n",
    "        fx, Δt, Σf = fx_matriz[i,j], Δt_matriz[i,j], Σf_matriz[i,j]\n",
    "        if fx < fmin + abs(fmin) * epsR + epsA\n",
    "            @printf(\"%+8.2e  %8.2e  %6d | \", fx, Δt, Σf)\n",
    "        else\n",
    "            @printf(\"%27s | \", \"Falhou\")\n",
    "        end\n",
    "    end\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro ponto de comparação é a velocidade de Newton quando funciona. Ele faz uma troca de **eficiência** por **robustez**. Isso fica pouco evidente na tabela acima, então vamos usar um gráfico específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfil de Desempenho\n",
    "\n",
    "O perfil de desempenho é um gráfico de comparação de algoritmos útil quando existe uma troca de eficiência e robustez. Em geral ele não é muito útil para algoritmos que sempre convergem.\n",
    "\n",
    "A ideia do perfil de desempenho é \"normalizar\" a comparação. Problemas menores tendem a ser resolvidos mais rápido, enquanto problemas maiores podem demorar vários minutos. Nessa situação, 1 minuto de diferença pode ser muito ou pouco.\n",
    "\n",
    "A normalização do perfil se dá pela divisão de cada custo pelo menor custo. Por exemplo, imagine que a matriz $T$ abaixo é uma matriz de comparação de tempo de 3 algoritmos (como $\\Delta t$ acima). Cada linha corresponde a um problema diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = rand(15, 3) .* (1:15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui podemos ter algumas problemas que não convergem. Podemos denotá-los colocando $\\infty$ nos valores correspondentes em $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T[[7;11;13;19;23]] = Inf\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O menor tempo de cada problema é calculado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tmin = minimum(T, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daí a razão é calculada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = T ./ Tmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que sempre temos ao menos um problema com custo relativo $1$, a não ser que todos os problemas falhem.\n",
    "\n",
    "A partir destes custos, definimos a função do perfil:\n",
    "$$ \\rho_a(t) = \\dfrac{\\#\\{p \\ | \\ r_{a,p} \\leq t\\}}{NP}, $$\n",
    "onde $r_{a,p}$ é a razão do problema $p$ do algoritmo $a$, e $NP$ é o número de problemas.\n",
    "Em outras palavras, $\\rho_a(t)$ mede, para o algoritmo $a$, quantos problemas foram resolvidos com custo relativo até $t$. Veja que $t = 1$ é o menor possível, então representa quantos problemas o algoritmo $a$ resolveu mais rápido que os adversários. Para $t \\to \\infty$, $\\rho_a(t)$ corresponde a quantos problemas o algoritmo $a$ resolveu. Definimos esses dois valores:\n",
    "- Eficiência: $\\rho_a(1)$;\n",
    "- Robustez: $\\displaystyle\\lim_{t \\to \\infty} \\rho_a(t)$.\n",
    "\n",
    "Note que $\\rho_a$ vai de $0$ a $1$, e que é descontínua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "τ = sort(unique(R))\n",
    "\n",
    "if τ[end] == Inf\n",
    "    τ = τ[1:end-1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np = size(R, 1)\n",
    "P = zeros(length(τ), 3)\n",
    "for (i,τi) = enumerate(τ)\n",
    "    for j = 1:3\n",
    "        P[i,j] = sum(R[:,j] .<= τi) / np\n",
    "    end\n",
    "end\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "gr()\n",
    "\n",
    "plot(τ, P, t=:steppre, leg=:bottomright, lab=[\"Alg1\", \"Alg2\", \"Alg3\"])\n",
    "ylims!(0, 1)\n",
    "\n",
    "plot(τ, P, t=:steppre, leg=:bottomright, lab=[\"Alg1\", \"Alg2\", \"Alg3\"], xaxis=:log)\n",
    "ylims!(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pacote `BenchmarkProfiles` tem um perfil de desempenho implementado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkProfiles\n",
    "\n",
    "performance_profile(T, [\"Alg1\", \"Alg2\", \"Alg3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfil de desempenho dos algoritmos que fizemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsR, epsA = 1e-3, 1e-6\n",
    "np = size(fx_matriz, 1)\n",
    "funcionou = fill(false, np, nmtds)\n",
    "for i = 1:np\n",
    "    fmin = minimum(fx_matriz[i,:])\n",
    "    for j = 1:nmtds\n",
    "        fx, Δt, Σf = fx_matriz[i,j], Δt_matriz[i,j], Σf_matriz[i,j]\n",
    "        funcionou[i,j] = fx < fmin + abs(fmin) * epsR + epsA\n",
    "    end\n",
    "end\n",
    "\n",
    "P = Δt_matriz + (.!funcionou) * Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = Δt_matriz + (.!funcionou) * Inf # Truques\n",
    "performance_profile(P, string.(mtds), leg=:bottomright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = Σf_matriz + (.!funcionou) * Inf\n",
    "performance_profile(P, string.(mtds), leg=:bottomright)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
